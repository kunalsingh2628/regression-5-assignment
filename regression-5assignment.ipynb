{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeeadb6c-97be-4186-b00b-a8f7f8dbbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e55587-963c-427c-8616-c20f08d4d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic Net Regression is a type of linear regression that combines the penalties of both L1 (Lasso) and L2 (Ridge) regularization methods. It is used for variable selection and addressing multicollinearity in regression models. Elastic Net introduces two parameters, alpha (α) and lambda (λ), which control the strength of the L1 and L2 regularization terms, respectively.\n",
    "\n",
    "The objective function of Elastic Net Regression is given by:\n",
    "\n",
    "Minimize\n",
    "[\n",
    "1\n",
    "2\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    ")\n",
    "2\n",
    "+\n",
    "�\n",
    "⋅\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "1\n",
    "2\n",
    "⋅\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "�\n",
    "2\n",
    "+\n",
    "�\n",
    "⋅\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    ")\n",
    "]\n",
    "Minimize[ \n",
    "2N\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "N\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +α⋅λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ( \n",
    "2\n",
    "1\n",
    "​\n",
    " ⋅(1−α)⋅β \n",
    "j\n",
    "2\n",
    "​\n",
    " +α⋅∣β \n",
    "j\n",
    "​\n",
    " ∣)]\n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "N is the number of observations.\n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  is the observed response for the \n",
    "�\n",
    "i-th observation.\n",
    "�\n",
    "^\n",
    "�\n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    "  is the predicted response for the \n",
    "�\n",
    "i-th observation.\n",
    "�\n",
    "p is the number of predictors.\n",
    "�\n",
    "�\n",
    "β \n",
    "j\n",
    "​\n",
    "  is the coefficient for the \n",
    "�\n",
    "j-th predictor.\n",
    "The term \n",
    "1\n",
    "2\n",
    "⋅\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "�\n",
    "2\n",
    "2\n",
    "1\n",
    "​\n",
    " ⋅(1−α)⋅β \n",
    "j\n",
    "2\n",
    "​\n",
    "  corresponds to the Ridge regularization, and \n",
    "�\n",
    "⋅\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "α⋅∣β \n",
    "j\n",
    "​\n",
    " ∣ corresponds to the Lasso regularization. The parameters \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ control the trade-off between the L1 and L2 penalties and the overall strength of regularization.\n",
    "\n",
    "Differences from other regression techniques:\n",
    "\n",
    "Lasso Regression (L1 Regularization):\n",
    "\n",
    "Only includes the L1 penalty term in the objective function.\n",
    "Tends to produce sparse coefficient vectors, effectively setting some coefficients to exactly zero, leading to variable selection.\n",
    "Suitable for feature selection when there are many irrelevant or redundant predictors.\n",
    "Ridge Regression (L2 Regularization):\n",
    "\n",
    "Only includes the L2 penalty term in the objective function.\n",
    "Does not yield sparse coefficient vectors; it shrinks the coefficients toward zero but rarely exactly to zero.\n",
    "Effective in handling multicollinearity by preventing the coefficients from becoming too large.\n",
    "Elastic Net Regression:\n",
    "\n",
    "Combines both L1 and L2 penalties, offering a compromise between Lasso and Ridge.\n",
    "Addresses both the sparsity-inducing property of Lasso and the multicollinearity-handling property of Ridge.\n",
    "Provides more flexibility in handling various types of datasets, especially when there are many predictors and some of them are correlated.\n",
    "Elastic Net is particularly useful when dealing with high-dimensional datasets where the number of predictors is large compared to the number of observations. The choice of \n",
    "�\n",
    "α allows users to control the balance between Lasso and Ridge regularization based on the specific characteristics of the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138860e6-a0ea-463e-acc0-bb7503915ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffe6f1-07fd-417e-917e-5eb2fd7b6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing the optimal values for the regularization parameters (\n",
    "�\n",
    "α and \n",
    "�\n",
    "λ) in Elastic Net Regression is crucial for achieving a well-performing model. The process often involves using techniques like cross-validation to assess the model's performance with different parameter values. Here's a general approach:\n",
    "\n",
    "Grid Search:\n",
    "\n",
    "Define a grid of potential values for \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ. Typically, a range of values covering both extremes (pure Lasso and pure Ridge) is considered.\n",
    "Perform a grid search, fitting the Elastic Net model with each combination of \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ.\n",
    "Evaluate the model performance using a suitable metric (e.g., mean squared error for regression problems) on a validation set.\n",
    "Cross-Validation:\n",
    "\n",
    "Use k-fold cross-validation to assess model performance for each combination of \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ.\n",
    "Split the dataset into \n",
    "�\n",
    "k folds and train the model on \n",
    "�\n",
    "−\n",
    "1\n",
    "k−1 folds, validating on the remaining fold. Repeat this process \n",
    "�\n",
    "k times, rotating the validation fold each time.\n",
    "Compute the average performance metric across all folds for each combination of \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ.\n",
    "Select Optimal Parameters:\n",
    "\n",
    "Choose the combination of \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ that yields the best performance on the validation set.\n",
    "The definition of \"best performance\" depends on the specific problem and metric, but it's often related to minimizing prediction error.\n",
    "Test Set Evaluation:\n",
    "\n",
    "After selecting the optimal parameters based on cross-validation, assess the model's performance on a separate test set that was not used during the parameter tuning process.\n",
    "This provides an unbiased estimate of how well the model generalizes to new, unseen data.\n",
    "Regularization Path Algorithms:\n",
    "\n",
    "Some optimization algorithms for Elastic Net, such as the coordinate descent algorithm, can efficiently compute solutions for a range of \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ values, allowing for the exploration of the entire regularization path.\n",
    "Regularization path algorithms can be useful for understanding how the coefficients change with different levels of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17817fb-345d-4551-9325-42e413cc77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327e025-d94e-4c71-bc70-5cc7ed9dc970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Advantages of Elastic Net Regression:\n",
    "\n",
    "Handles Multicollinearity:\n",
    "\n",
    "Elastic Net Regression is effective in handling multicollinearity, a situation where predictor variables are highly correlated. The combination of L1 and L2 regularization helps prevent overfitting and stabilizes the model in the presence of correlated predictors.\n",
    "Variable Selection:\n",
    "\n",
    "Like Lasso Regression, Elastic Net has the ability to perform variable selection by driving some of the coefficients to exactly zero. This can be advantageous when dealing with high-dimensional datasets with many irrelevant or redundant predictors.\n",
    "Flexibility:\n",
    "\n",
    "Elastic Net provides a flexible framework that allows users to balance between L1 (Lasso) and L2 (Ridge) regularization. The parameter \n",
    "�\n",
    "α controls the trade-off, providing adaptability to the specific characteristics of the data.\n",
    "Suitable for High-Dimensional Data:\n",
    "\n",
    "Well-suited for situations where the number of predictors is large compared to the number of observations. It is particularly useful in high-dimensional datasets commonly encountered in fields like genomics, finance, and text mining.\n",
    "Robustness:\n",
    "\n",
    "Generally more robust than Lasso Regression alone because the inclusion of Ridge regularization helps to mitigate some of the limitations of Lasso, such as sensitivity to outliers.\n",
    "Disadvantages of Elastic Net Regression:\n",
    "\n",
    "Interpretability:\n",
    "\n",
    "The model may be less interpretable compared to simple linear regression because the combination of L1 and L2 penalties can make it challenging to interpret the individual impact of each predictor variable.\n",
    "Computationally Intensive:\n",
    "\n",
    "Training an Elastic Net model can be computationally more intensive than standard linear regression, especially when dealing with a large number of predictors. The optimization process involves solving a convex optimization problem, and certain algorithms might be needed to handle this efficiently.\n",
    "Parameter Tuning:\n",
    "\n",
    "Determining the optimal values for the regularization parameters (\n",
    "�\n",
    "α and \n",
    "�\n",
    "λ) requires careful tuning. This process may involve grid search and cross-validation, which can be computationally demanding.\n",
    "Not Always Necessary:\n",
    "\n",
    "In situations where multicollinearity is not a significant issue or when the number of predictors is relatively small, simpler models like ordinary least squares (OLS) regression might be sufficient and more interpretable.\n",
    "Risk of Over-Regulation:\n",
    "\n",
    "Depending on the choice of parameters, Elastic Net may shrink coefficients too aggressively, leading to underfitting if the regularization is too strong. It is important to strike a balance to prevent excessive penalty on the coefficients.\n",
    "In summary, Elastic Net Regression is a powerful technique that addresses some of the limitations of individual Lasso and Ridge regression methods. Its advantages include handling multicollinearity, providing variable selection, and offering flexibility. However, it comes with challenges related to interpretability, computational intensity, and the need for parameter tuning. The choice of the regularization parameters requires careful consideration based on the specific characteristics of the data and the goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0d323-6d77-48c3-ba3c-f6e686f82022",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6349af2e-50f3-4e61-b9e2-9193f2bf93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "High-Dimensional Datasets:\n",
    "\n",
    "Elastic Net is particularly useful when dealing with datasets where the number of predictors is much larger than the number of observations. This is common in fields such as genomics, finance, and text mining.\n",
    "Multicollinearity:\n",
    "\n",
    "When there is a high degree of multicollinearity among predictor variables, Elastic Net can help address this issue by combining the L1 and L2 penalties. It is effective in stabilizing the regression coefficients and preventing overfitting.\n",
    "Variable Selection:\n",
    "\n",
    "Elastic Net, with its L1 penalty, is suitable for situations where variable selection is important. It can automatically shrink some coefficients to zero, effectively performing feature selection and simplifying the model.\n",
    "Regularization in Regression:\n",
    "\n",
    "When traditional linear regression methods are prone to overfitting, Elastic Net provides a regularization framework that balances the bias-variance trade-off, preventing the model from becoming too complex.\n",
    "Predictive Modeling:\n",
    "\n",
    "Elastic Net is commonly used for predictive modeling tasks, especially in scenarios where a parsimonious model is desired, and there is a need to avoid overfitting.\n",
    "Biomedical Research:\n",
    "\n",
    "In genomics and other biomedical research, where datasets often have a large number of genetic markers or features, Elastic Net can help identify relevant predictors and build predictive models.\n",
    "Finance:\n",
    "\n",
    "In finance, where predicting stock prices or financial outcomes often involves dealing with a large number of variables, Elastic Net can be employed to build robust and stable models.\n",
    "Marketing and Customer Analytics:\n",
    "\n",
    "Elastic Net can be used in marketing and customer analytics to model consumer behavior and make predictions based on various features such as demographics, purchasing history, and online behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1d0f8-3209-461d-8dac-36293fa70b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffc302-d62e-455d-b74b-caa974f58f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interpreting coefficients in Elastic Net Regression is somewhat similar to interpreting coefficients in standard linear regression, but there are additional considerations due to the regularization terms. Here's a general guide:\n",
    "\n",
    "Sign and Magnitude:\n",
    "\n",
    "The sign of a coefficient indicates the direction of the relationship between the predictor variable and the response variable. The magnitude represents the strength of the relationship. Larger magnitudes suggest a stronger impact on the response.\n",
    "Coefficient Shrinkage:\n",
    "\n",
    "Due to the regularization terms (L1 and L2 penalties), some coefficients in Elastic Net may be shrunken towards zero. A coefficient that is exactly zero implies that the corresponding predictor has been effectively excluded from the model. This is a form of automatic variable selection.\n",
    "Importance of Non-Zero Coefficients:\n",
    "\n",
    "Focus on the non-zero coefficients, as they indicate the variables that are deemed important by the model. These are the predictors that contribute to the model's predictions.\n",
    "Trade-off between Lasso and Ridge:\n",
    "\n",
    "The trade-off between L1 (Lasso) and L2 (Ridge) regularization is controlled by the parameter \n",
    "�\n",
    "α. A higher \n",
    "�\n",
    "α places more emphasis on Lasso, potentially leading to more coefficients being exactly zero.\n",
    "Relative Importance:\n",
    "\n",
    "Compare the magnitudes of coefficients to assess their relative importance. Be cautious about comparing coefficients across different scales, as the regularization terms may affect their absolute values.\n",
    "Interaction Effects:\n",
    "\n",
    "Consider interaction effects between predictors. The impact of a predictor may depend on the presence of other predictors, and Elastic Net can provide insights into how the model handles such interactions.\n",
    "Domain Knowledge:\n",
    "\n",
    "Incorporate domain knowledge to aid in the interpretation. Understanding the context of the problem can help make sense of the relationships captured by the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2798649-b45d-44d3-9380-0d5e0d2e45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967ccc8-2888-430b-93bf-ee5196a6ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling missing values is an important step in any machine learning or statistical modeling process, including Elastic Net Regression. The way you handle missing values can impact the performance and interpretability of your model. Here are several strategies to deal with missing values when using Elastic Net Regression:\n",
    "\n",
    "Remove Rows with Missing Values:\n",
    "\n",
    "One straightforward approach is to remove observations (rows) that contain missing values. However, this can result in a loss of valuable information, especially if the missing values are not randomly distributed.\n",
    "Imputation:\n",
    "\n",
    "Imputation involves replacing missing values with estimated or predicted values. Common imputation methods include mean imputation, median imputation, or using more advanced techniques like k-nearest neighbors imputation or regression imputation. Choose the method that is appropriate for your data and the nature of missingness.\n",
    "Indicator/Dummy Variables:\n",
    "\n",
    "Create indicator variables (dummy variables) to flag the missing values in categorical predictors. This allows the model to learn the impact of missing values as a separate category. For numerical predictors, you can create a binary indicator variable indicating whether the value is missing or not.\n",
    "Predictive Imputation:\n",
    "\n",
    "Use machine learning techniques to predict missing values based on other variables. This can involve training a separate model (such as a regression model) to predict the missing values using the available information.\n",
    "Elastic Net with Missing Values:\n",
    "\n",
    "Some implementations of Elastic Net Regression can handle missing values directly. In such cases, the algorithm automatically adapts to the missing data during the training process. It's important to check the documentation of the specific implementation or library you are using to see if this functionality is supported.\n",
    "Multiple Imputation:\n",
    "\n",
    "Conduct multiple imputations to create several complete datasets with different imputed values for missing data. Fit the Elastic Net model on each imputed dataset and then combine the results to obtain more robust estimates. This approach accounts for the uncertainty introduced by imputing missing values.\n",
    "Domain-Specific Imputation:\n",
    "\n",
    "Depending on the context of your data, domain-specific knowledge might guide the imputation process. For example, if missing values are related to a specific condition or circumstance, you may have insights on how to impute those values more accurately.\n",
    "Handling Missing Target Values:\n",
    "\n",
    "If the response variable (target) has missing values, you may consider treating it as a separate category or using techniques like regression imputation to predict the missing target values based on other predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febdb3e-b45a-415a-bc73-15cc58f0e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbf5b2-967a-48e8-b18f-4fd523d81a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic Net Regression can be a powerful tool for feature selection, as it has the ability to automatically shrink some coefficients to exactly zero, effectively excluding certain predictors from the model. Here's how you can use Elastic Net Regression for feature selection:\n",
    "\n",
    "Set Up the Elastic Net Model:\n",
    "\n",
    "Define your response variable (dependent variable) and predictor variables (independent variables).\n",
    "Specify the Elastic Net model, including setting the values for the regularization parameters \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ. The parameter \n",
    "�\n",
    "α controls the balance between L1 (Lasso) and L2 (Ridge) regularization.\n",
    "Train the Model:\n",
    "\n",
    "Train the Elastic Net model on your training dataset using the specified values for \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ. Ensure that you use appropriate techniques for handling missing values and preprocessing the data.\n",
    "Analyze Coefficients:\n",
    "\n",
    "Examine the coefficients obtained from the fitted Elastic Net model. Coefficients that are exactly zero indicate that the corresponding predictors have been excluded from the model. These predictors are effectively selected out during the regularization process.\n",
    "Identify Important Predictors:\n",
    "\n",
    "Focus on the non-zero coefficients, as they correspond to the predictors that the model considers important. These are the features that contribute to the model's predictions.\n",
    "Adjust \n",
    "�\n",
    "α for Feature Selection:\n",
    "\n",
    "Adjust the value of \n",
    "�\n",
    "α to control the strength of the L1 penalty. Higher values of \n",
    "�\n",
    "α increase the sparsity of the solution, leading to more coefficients being exactly zero. Experiment with different values to find the optimal level of sparsity for your specific problem.\n",
    "Cross-Validation for Model Selection:\n",
    "\n",
    "Use cross-validation to select the optimal values for \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ. This helps in finding a balance between sparsity and predictive performance. Conduct a grid search over different values of \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ and choose the combination that provides the best performance on a validation set.\n",
    "Regularization Path:\n",
    "\n",
    "Some implementations of Elastic Net provide a regularization path, which shows how the coefficients change for different values of \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ. Analyzing the regularization path can give insights into the feature selection process.\n",
    "Post-Selection Inference:\n",
    "\n",
    "If feature selection is a primary goal, consider using methods for post-selection inference to account for the uncertainty introduced by the feature selection process. This helps in obtaining more reliable statistical inferences.\n",
    "It's important to note that the choice of \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ should be based on both the goal of feature selection and the desire for predictive accuracy. Setting \n",
    "�\n",
    "α too high may result in too many features being excluded, potentially leading to an underfit model. Therefore, the tuning of \n",
    "�\n",
    "α and \n",
    "�\n",
    "λ should be guided by a balance between sparsity and model performance. Additionally, interpreting the selected features should be done in the context of the specific problem and domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1fa68-9b2c-4fa1-9f35-f8ce8357018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257043f-c57d-4d65-a9dd-843f88653c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Python, the pickle module is commonly used for serializing and deserializing objects, including machine learning models. \n",
    "Here a simple example of how you can pickle and unpickle a trained Elastic Net Regression model using the pickle module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a0b8a-efb5-4341-97b1-a183788aa257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4e311-b91e-41dc-8c92-45cecdc42162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87d638-b72d-4a4a-8b4b-fd3594c11709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ecc89-11a2-49f6-b4e0-ef4e89fd9dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623d22b-a8ef-4f5c-a5a9-218182b7d283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cacc52-226c-4539-8383-bc06442d8e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dda4df-4b80-4695-85ab-b8cc95694e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8f7c8-3a49-4a1e-bc90-eab7cf6a1868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a436b-a542-4555-b308-b0bcab39bb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8745473-8c24-4235-80fc-69798af121b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eec1a7-4ab6-4d5a-aca7-f483632c9230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3e028-3387-40ab-8e85-6810402824fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
